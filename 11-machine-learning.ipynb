{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import library_data_science as lds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Multi-Dimentional Data\n",
    "\n",
    "\n",
    "Multi-dimensional data can be represented as an array of tuples, where each tuple consists one, two or more elements. In this structure, the first element in each tuple is treated as independent, while the another elements typically depends on the first, reflecting the relationship between the variables. \\\n",
    "Remember that `D.size = n` and $\\forall{m < n}($ `D[m].size = k`$)$.\n",
    "\n",
    "$$D =  \\bigg< (d_{00}, d_{01}, ..., d_{0(k-1)}), (d_{10}, d_{11}, ..., d_{1(k-1)}), (d_{20}, d_{21}, ..., d_{2(k-1)}), ... , (d_{(n-1)0}, d_{(n-1)1}, ..., d_{(n-1)(k-1)}) \\bigg>,$$\n",
    "$$D = \\bigg< (D[0][0], D[0][1], ..., D[0][k-1]), (D[1][0], D[1][1], ..., D[1][k-1]), ..., (D[n-1][0], D[n-1][1], ..., D[n-1][k-1]) \\bigg>$$\n",
    "\n",
    "Multi-dimentional dataset can be unzipped to the $k$ separated sets.\n",
    "\n",
    "$$D_0 = \\big< d_{00}, d_{10}, ... , d_{(n-1)0} \\big> = \\big< D[0][0], D[1][0], ... , D[n-1][0] \\big> $$\n",
    "$$D_1 = \\big< d_{01}, d_{11}, ... , d_{(n-1)1} \\big> = \\big< D[0][1], D[1][1], ... , D[n-1][1] \\big>$$\n",
    "$$...$$\n",
    "$$D_{k-1} = \\big< d_{0(k-1)}, d_{1(k-1)}, ... , d_{(n-1)(k-1)} \\big> = \\big< D[0][k-1], D[1][k-1], ... , D[n-1][k-1] \\big>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the difference, I will use the example I used in the presentation to explain what multidimensional data are.\n",
    "\n",
    "#### Example 1: Runners' Performance\n",
    "\n",
    "When examining the distances covered by runners during a 5-minute run, along with their heart rates and oxygen consumption, our data will no longer be one-dimensional. Instead, it will consist of multiple attributes for each runner.\n",
    "\n",
    "$$\n",
    "Distance\\_Heart\\_Oxygen = \\big< (1078, 145, 3.2), (896, 152, 2.9), (1196, 138, 3.5), (1009, 149, 3.1), (1078, 143, 3.3), (1096, 141, 3.4), (923, 155, 3.0) \\big>\n",
    "$$\n",
    "\n",
    "Here, each tuple represents **(distance in meters, heart rate in bpm, oxygen consumption in L/min)**, making it a **three-dimensional dataset**.\n",
    "\n",
    "$$\n",
    "Distance = \\big<1078, 896, 1196, 1009, 1078, 1096, 923\\big> \\\\\n",
    "Heart = \\big<145, 152, 138, 149, 143, 141, 155\\big> \\\\\n",
    "Oxygen = \\big<3.2, 2.9, 3.5, 3.1, 3.3, 3.4, 3.0\\big>\n",
    "$$\n",
    "\n",
    "#### Example 2: Physiological and Lifestyle Factors\n",
    "\n",
    "When studying multiple physiological and lifestyle factors influencing body weight, a more complex dataset may include height, body weight, age, and daily caloric intake.\n",
    "\n",
    "$$\n",
    "Weights\\_Heights\\_Ages\\_Calories = \\big< (60, 177, 25, 2200), (76, 189, 30, 2500), (99, 197, 28, 2700), (48, 165, 22, 1800) \\big>\n",
    "$$\n",
    "\n",
    "Each entry now contains four attributes, making it a **four-dimensional dataset**.\n",
    "\n",
    "$$\n",
    "Weights = \\big< 60, 76, 99, 48 \\big> \\\\\n",
    "Heights = \\big< 177, 189, 197, 165 \\big> \\\\\n",
    "Ages = \\big< 25, 30, 28, 22 \\big> \\\\\n",
    "Calories = \\big< 2200, 2500, 2700, 1800 \\big>\n",
    "$$\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "The more attributes we include in our dataset, the higher its dimensionality. Multidimensional data allow for deeper analysis, such as finding correlations between different factors, but they also introduce challenges like increased complexity in visualization and computational processing. \n",
    "\n",
    "**Machine learning techniques, such as Principal Component Analysis (PCA), can help reduce dimensionality while preserving essential information.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "**Machine Learning (ML)** is a branch of artificial intelligence (AI) that enables computers to recognize patterns and make decisions based on data—without the need for explicitly programmed rules.\n",
    "\n",
    "![Traditional Programming versus Machine Learning](https://cdn.prod.website-files.com/614c82ed388d53640613982e/63ef5f4e24edde6ef055c3b2_traditional%20programming%20vs%20machine%20learning.jpg)\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "1. **Input Data** – The ML model receives a large amount of data, such as images, text, numbers, or sounds.\n",
    "\n",
    "2. **Training** – The algorithm analyzes the data and \"learns\" relationships between them, adjusting its parameters.\n",
    "\n",
    "3. **Prediction** – After training, the model can process new data and make decisions based on it.\n",
    "\n",
    "### Examples of ML applications:\n",
    "\n",
    "- **Recommendations** (Netflix, Spotify suggesting movies/music)\n",
    "\n",
    "- **Speech and image recognition** (Siri, Google Lens)\n",
    "\n",
    "- **Spam filters** (detecting spam emails)\n",
    "\n",
    "- **Predictive systems** (weather forecasts, financial analysis)\n",
    "\n",
    "### Types of Machine Learning:\n",
    "\n",
    "1. **Supervised Learning** – The model learns from labeled data (e.g., images of cats and dogs, where it knows what is what).\n",
    "\n",
    "2. **Unsupervised Learning** – The model searches for patterns in data without labels (e.g., clustering customers based on similar behavior).\n",
    "\n",
    "![Supervised versus Unsupervised Learning](https://www.mathworks.com/discovery/machine-learning/_jcr_content/mainParsys/band/mainParsys/lockedsubnav/mainParsys/columns/a32c7d5d-8012-4de1-bc76-8bd092f97db8/image_2128876021_cop.adapt.full.medium.svg/1741205964325.svg)\n",
    "\n",
    "### Basic Paradigm of Machine Learning\n",
    "\n",
    "1. Observe set of examples: **training data**.\n",
    "\n",
    "2. Infer something about process that generated that data.\n",
    "\n",
    "3. Use inference to make predictions about previously unseen data: **test data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised versus Unsupervised Learning\n",
    "\n",
    "Machine learning can be broadly categorized into two main types: **Supervised Learning** and **Unsupervised Learning**. The key difference lies in whether the data used for training includes labeled outputs.\n",
    "\n",
    "![Classification versus Clustering versus Regression](https://lh6.googleusercontent.com/proxy/b9cTY0TniOxMDzL0UEDPN9WdCMqxJ0ETnubKDQ37IIubX6NK1l_iGMkRZTzAdC-Xi3G2V9_jX9PlAQzsUd2g-LLxU7q0qM_KgzKiOeuIodms5uNEVQoy0xEw93U75fZPVT-R_-XN7D4h5L6E)\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Supervised learning is a type of machine learning where the model learns from **labeled data**. Each training example consists of an input and a corresponding correct output.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The algorithm is trained on a dataset containing **inputs (X)** and **expected outputs (Y)**.\n",
    "\n",
    "- The model makes predictions and adjusts itself based on the difference between its predictions and the actual labels.\n",
    "\n",
    "- Once trained, the model can make accurate predictions on new, unseen data.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "- **Email Spam Detection** – Given labeled emails (\"spam\" or \"not spam\"), the model learns to classify new emails.\n",
    "\n",
    "- **Image Classification** – Identifying whether an image contains a cat or a dog.\n",
    "\n",
    "- **Stock Price Prediction** – Predicting future stock prices based on historical labeled data.\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised learning deals with **unlabeled data**. The model finds patterns and structures in the data without predefined labels.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "- The algorithm analyzes input data **without any associated outputs**.\n",
    "\n",
    "- It groups similar data points or identifies hidden structures.\n",
    "\n",
    "- Often used for **clustering**, **anomaly detection**, and **pattern recognition**.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "- **Customer Segmentation** – Grouping customers by purchasing behavior.\n",
    "\n",
    "- **Anomaly Detection** – Identifying fraudulent transactions in banking.\n",
    "\n",
    "- **Topic Modeling** – Discovering topics in a collection of documents.\n",
    "\n",
    "\n",
    "### Key Differences: \n",
    "| Feature              | Supervised Learning | Unsupervised Learning |\n",
    "|----------------------|--------------------|----------------------|\n",
    "| **Data Type**        | Labeled data (X, Y) | Unlabeled data (X) |\n",
    "| **Main Goal**        | Learn a mapping from inputs to outputs | Find hidden structures and patterns |\n",
    "| **Typical Use Cases** | Classification, Regression | Clustering, Anomaly Detection |\n",
    "\n",
    "![Classification versus Clustering](https://cdn.prod.website-files.com/614c82ed388d53640613982e/63ef769f6a877d715fa75008_supervised%20vs%20Unsupervised%20learning.jpg)\n",
    "\n",
    "Both types of learning have unique applications and are used depending on the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Process of creating, transforming and selecting features used as input for machine learning models. The goal is to improve the quality of the data so the model can learn and make predictions more effectively.\n",
    "\n",
    "### 1. Understanding the Problem and Data  \n",
    "\n",
    "- **Domain Knowledge** – Understand which features might influence the outcome.  \n",
    "\n",
    "- **Exploratory Data Analysis (EDA)** – Identify missing values, outliers, and relationships.  \n",
    "\n",
    "- **Understanding Variable Types** – Identify numerical, categorical, and time-based data.  \n",
    "\n",
    "\n",
    "### 2. Feature Selection and Transformation  \n",
    "\n",
    "- **Removing Irrelevant Features** – Drop low-variance or unrelated columns.  \n",
    "\n",
    "- **Handling Missing Values** – Impute (mean, median, mode), remove, or replace with special values.  \n",
    "\n",
    "- **Normalization & Standardization** – Use min-max scaling or standard scaling for models sensitive to scale.  \n",
    "\n",
    "### 3. Creating New Features  \n",
    "\n",
    "- **Feature Extraction** – Generate new variables from existing ones (e.g., date differences, ratios).  \n",
    "\n",
    "- **Binning Values** – Categorize numerical values into discrete bins (e.g., age groups).  \n",
    "\n",
    "- **Feature Interactions** – Create new features by multiplying or dividing existing ones (e.g., price per unit).  \n",
    "\n",
    "### 4. Transforming Categorical Variables  \n",
    "\n",
    "- **One-Hot Encoding** – Convert categories into binary vectors.  \n",
    "\n",
    "- **Label Encoding** – Assign integer values to categories.  \n",
    "\n",
    "- **Target Encoding** – Replace categories with the mean target value (use with caution to avoid overfitting).  \n",
    "\n",
    "### 5. Dimensionality Reduction and Redundancy Removal  \n",
    "\n",
    "- **Removing Highly Correlated Features** – Avoid collinearity.  \n",
    "\n",
    "- **PCA (Principal Component Analysis)** – Reduce dimensionality when dealing with too many features.  \n",
    "\n",
    "- **Feature Selection (e.g., Lasso, ANOVA, statistical tests)** – Choose the most relevant features.  \n",
    "\n",
    "### 6. Validation and Testing  \n",
    "\n",
    "- **Measuring Feature Impact** – Compare model metrics before and after adding features.  \n",
    "\n",
    "- **Avoiding Data Leakage** – Ensure features are created only from available information (no future data usage).  \n",
    "\n",
    "- **Using Cross-Validation** – Verify the stability of new features.  \n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Overfitting occurs when a machine learning model learns the training data too well, capturing noise and random fluctuations instead of general patterns. As a result, the model performs exceptionally well on the training data but fails to generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Statistical Metrics in Machine Learning\n",
    "\n",
    "When evaluating machine learning models, we use various statistical measures to assess their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Confusion Matrix**: A table that summarizes the performance of a classification model by comparing actual vs. predicted values.\n",
    "\n",
    "  | Observed / Predicted | $c_1$            | $c_2$            | $c_3$            | ... | $c_k$            |\n",
    "  |----------------------|------------------|------------------|------------------|-----|------------------|\n",
    "  | $c_1$                | $Tc_1$           | $Fc_2$ for $c_1$ | $Fc_3$ for $c_1$ | ... | $Fc_k$ for $c_1$ |\n",
    "  | $c_2$                | $Fc_1$ for $c_2$ | $Tc_2$           | $Fc_3$ for $c_2$ | ... | $Fc_k$ for $c_2$ |\n",
    "  | $c_3$                | $Fc_1$ for $c_3$ | $Fc_2$ for $c_3$ | $Tc_3$           | ... | $Fc_k$ for $c_3$ |\n",
    "  | ...                  | ...              | ...              | ...              | ... | ...              |\n",
    "  | $c_k$                | $Fc_1$ for $c_k$ | $Fc_2$ for $c_k$ | $Fc_3$ for $c_k$ | ... | $Tc_k$           |\n",
    "\n",
    "* **Accuracy**: Measures the overall correctness of the model. It is the ratio of correctly classified instances to the total instances. However, it can be misleading if the dataset is imbalanced.  \n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{\\sum_{i=1}^{k}{Tc_i}}{\\sum_{i=1}^{k}{Tc_i} + \\sum_{i=1}^{k}{\\sum_{j=1}^{k}{\\big(Fc_{i}\\text{ for } c_{j}}}\\big)}\n",
    "$$\n",
    "\n",
    "* **Sensitivity (Recall or True Positive Rate)**: Measures how well the model identifies positive instances. Higher sensitivity means fewer false negatives.  \n",
    "\n",
    "$$\n",
    "Recall(c_{i}) = \\frac{Tc_{i}}{Tc_{i} + \\sum_{j=1}^{k}{\\big(Fc_{j} \\text{ for } c_{i}\\big)}}\n",
    "$$\n",
    "\n",
    "* **Positive Predictive Value (Precision)**: Represents the proportion of correctly predicted positive cases among all predicted positive cases. A higher precision means fewer false positives.  \n",
    "\n",
    "$$\n",
    "Precision(c_{i}) = \\frac{Tc_{i}}{Tc_{i} + \\sum_{j=1}^{k}{\\big(Fc_{i} \\text{ for } c_{j}\\big)}}\n",
    "$$\n",
    "\n",
    "* F1\n",
    "\n",
    "$$\n",
    "F1(c_{i}) = \\frac{2 \\times Precision(c_i) \\times Recall(c_i)}{Precision(c_i)+ Recall(c_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary-Class Classification Metrics\n",
    "\n",
    "\n",
    "* **Confusion Matrix**: A table that summarizes the performance of a classification model by comparing actual vs. predicted values. It consists of four components:\n",
    "\n",
    "  - **True Positives (TP)**: Correctly predicted positive cases.\n",
    "\n",
    "  - **False Positives (FP)**: Incorrectly predicted positive cases.\n",
    "\n",
    "  - **False Negatives (FN)**: Incorrectly predicted negative cases.\n",
    "\n",
    "  - **True Negatives (TN)**: Correctly predicted negative cases.\n",
    "\n",
    "  | Observed / Predicted | Positive | Negative |\n",
    "  |---|---|---|\n",
    "  | **Positive** | $TP$ | $FN$ |\n",
    "  | **Negative** | $FP$ | $TN$ |\n",
    "\n",
    "  $$\n",
    "  Confusion\\_Matrix = \\left[\\begin{array}{cc} TP & FN \\\\ FP & TN \\end{array}\\right]\n",
    "  $$\n",
    "  \n",
    "* **Accuracy**: Measures the overall correctness of the model. It is the ratio of correctly classified instances to the total instances. However, it can be misleading if the dataset is imbalanced.  \n",
    "  $$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "  \n",
    "* **Sensitivity (Recall or True Positive Rate)**: Measures how well the model identifies positive instances. Higher sensitivity means fewer false negatives.  \n",
    "  $$Sensitivity = \\frac{TP}{TP + FN}$$\n",
    "  \n",
    "* **Specificity (True Negative Rate)**: Measures how well the model identifies negative instances. Higher specificity means fewer false positives.  \n",
    "  $$Specificity = \\frac{TN}{TN + FP}$$\n",
    "  \n",
    "* **Positive Predictive Value (Precision)**: Represents the proportion of correctly predicted positive cases among all predicted positive cases. A higher precision means fewer false positives.  \n",
    "  $$PPV = \\frac{TP}{TP + FP}$$\n",
    "  \n",
    "* **Negative Predictive Value (NPV)**: Represents the proportion of correctly predicted negative cases among all predicted negative cases.  \n",
    "  $$NPV = \\frac{TN}{TN + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_classes(C_observed: list, C_predicted: list) -> list:\n",
    "    classes = []\n",
    "\n",
    "    for c in C_observed:\n",
    "        if c not in classes:\n",
    "            classes.append(c)\n",
    "\n",
    "    for c in C_predicted:\n",
    "        if c not in classes:\n",
    "            classes.append(c)\n",
    "    \n",
    "    return classes\n",
    "\n",
    "def confusion_matrix(C_observed: list, C_predicted:list) -> list:\n",
    "    classes = count_classes(C_observed, C_predicted)\n",
    "\n",
    "    indexes = {}\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
